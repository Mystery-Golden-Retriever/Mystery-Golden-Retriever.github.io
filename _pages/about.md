---
permalink: /
title: "Welcome pages"
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

# Welcome
<span class='anchor' id='about-me'></span>

<!---I am currently a Ph.D. student in Computer Engineering at Information Sciences Institute (ISI) and ECE department of University of Southern California (USC). Prior to that, I completed my Master's degree in School of Electronic Science and Engineering, and my Bachelor's degree in Kuang Yaming Honors School (formerly Gifted Young Class), both from Nanjing University.


My research interests primarily focus on Efficient Deep Learning Algorithms, Machine Learning System, and Distributed Edge Computing. I am passionate about exploring new techniques and approaches to improve the performance and efficiency of deep learning models, making them more accessible and practical for real-world applications. I have published many papers at the top AI conferences with <strong><span id='total_cit'>72</span></strong></a> <a href='https://scholar.google.com/citations?user=AQuXJEkAAAAJ&hl=zh-CN'>google scholar citations.
(You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>).--->

I am currently a Ph.D. student in Computer Engineering at the Information Sciences Institute (ISI) and the ECE department of the University of Southern California (USC), advised by Prof. Stephen Crago. My dissertation research focuses on **Efficient and Trustworthy Distributed EdgeAI Systems in the Era of Large Language Models (LLM)**. 

Before joining USC, I completed my Master's degree in Electronic Engineering under Prof. Zhongfeng Wang (IEEE Fellow), and my Bachelor's degree in Physics from Kuang Yaming Honors School at Nanjing University.

My research interests span **Efficient Deep Learning Algorithms**, **Machine Learning Systems**, and **Distributed Edge Computing**. I am passionate about creating practical and efficient deep learning models and systems suitable for real-world applications. I have published papers in top AI conferences, accumulating **90** citations ([Google Scholar Profile](https://scholar.google.com/citations?user=AQuXJEkAAAAJ&hl=zh-CN)).


# üî• News
- *2025.04*: &nbsp;üéâüéâ Our paper **FedPaI** has been accepted by ICIC 2025 and released on arXiv.
- *2024.12*: &nbsp;üéâüéâ Present **MoQ** at NeurIPS 2024 Workshop MLNCP.
- *2024.05*: &nbsp;üéâüéâ Present **EFFICIENT AND TRUSTWORTHY DISTRIBUTED EDGEAI SYSTEM** at MLSys Young Professionals Symposium.
- *2023.06*: &nbsp;üéâüéâ Published papers **QuantPipe** and **BEBERT** at ICASSP 2023.


# üìù Publications 
<!-- <div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">arXiv 2025</div>
      <img src="images/fedpai.jpg" alt="FedPaI" width="70%">
    </div>
  </div>
  <div class='paper-box-text' markdown="1">
    [FedPaI: Achieving Extreme Sparsity in Federated Learning via Pruning at Initialization](https://arxiv.org/abs/2504.00308)

    <strong>Haonan Wang</strong>, Z Liu, K Hoshino, T Zhang, JP Walters, SP Crago

    - Introducing pruning at initialization for federated learning to significantly reduce computation and communication overhead.
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">NeurIPS 2024</div>
      <img src="images/moq.png" alt="MoQ" width="50%">
    </div>
  </div>
  <div class='paper-box-text' markdown="1">
    [MoQ: Mixture-of-format Activation Quantization for Communication-efficient AI Inference System](https://neurips.cc/virtual/2024/workshop/12345)

    <strong>Haonan Wang</strong>, Z Liu, C Fang, JP Walters, SP Crago

    - Proposed a mixed-format quantization method for AI inference, enhancing communication efficiency for edge/cloud deployments.
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">ICASSP 2023</div>
      <img src="images/quantpipe.png" alt="QuantPipe" width="100%">
    </div>
  </div>
  <div class='paper-box-text' markdown="1">
    [QuantPipe: Adaptive Post-Training Quantization for Distributed Transformer Pipelines in Dynamic Edge Environments](https://ieeexplore.ieee.org/document/10096015)

    <strong>Haonan Wang</strong>, C Imes, S Kundu, PA Beerel, SP Crago, JP Walters

    - Developed adaptive post-training quantization for transformer models in dynamic distributed edge environments.
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">ICASSP 2023</div>
      <img src="images/bebert.png" alt="BEBERT" width="50%">
    </div>
  </div>
  <div class='paper-box-text' markdown="1">
    [BEBERT: Efficient and Robust Binary Ensemble BERT](https://ieeexplore.ieee.org/document/10095470)

    J Tian, C Fang, <strong>Haonan Wang</strong>, Z Wang

    - Created an efficient and robust binary ensemble BERT, significantly reducing computational overhead.
  </div>
</div>

- [Accelerating 3D CNN using 3D Fast Fourier Transform](https://ieeexplore.ieee.org/document/9401561), C Fang, L He, **Haonan Wang**, J Wei, Z Wang, **ISCAS 2021**
- [Temporal Residual Feature Learning for 3D CNN Action Recognition](https://ieeexplore.ieee.org/document/9273915), **Haonan Wang**, Y Mei, J Lin, Z Wang, **SiPS 2020**
- [Design Light-weight 3D CNN for Video Recognition](https://arxiv.org/abs/1905.13388), **Haonan Wang**, J Lin, Z Wang, **arXiv 2019**
- [Sparse-Winograd Accelerator for CNNs](https://ieeexplore.ieee.org/document/8683507), **Haonan Wang**, W Liu, T Xu, J Lin, Z Wang, **ICASSP 2019**
- [Efficient Reconfigurable Hardware Core for CNNs](https://ieeexplore.ieee.org/document/8645271), **Haonan Wang**, J Lin, Y Xie, B Yuan, Z Wang, **Asilomar 2018** -->

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">arXiv 2025</div>
      <img src="images/fedpai.jpg" alt="FedPaI" width="80%">
    </div>
  </div>
  <div class='paper-box-text'>
    <h3><a href="https://arxiv.org/abs/2504.00308">FedPaI: Achieving Extreme Sparsity in Federated Learning via Pruning at Initialization</a></h3>
    <p><strong>Haonan Wang</strong>, Z Liu, K Hoshino, T Zhang, JP Walters, SP Crago</p>
    <p>Introducing pruning at initialization for federated learning to significantly reduce computation and communication overhead.</p>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">NeurIPS 2024</div>
      <img src="images/moq.png" alt="MoQ" width="80%">
    </div>
  </div>
  <div class='paper-box-text'>
    <h3><a href="https://neurips.cc/virtual/2024/workshop/12345">MoQ: Mixture-of-format Activation Quantization for Communication-efficient AI Inference System</a></h3>
    <p><strong>Haonan Wang</strong>, Z Liu, C Fang, JP Walters, SP Crago</p>
    <p>Proposed a mixed-format quantization method for AI inference, enhancing communication efficiency for edge/cloud deployments.</p>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">ICASSP 2023</div>
      <img src="images/quantpipe.png" alt="QuantPipe" width="100%">
    </div>
  </div>
  <div class='paper-box-text'>
    <h3><a href="https://ieeexplore.ieee.org/document/10096015">QuantPipe: Adaptive Post-Training Quantization for Distributed Transformer Pipelines in Dynamic Edge Environments</a></h3>
    <p><strong>Haonan Wang</strong>, C Imes, S Kundu, PA Beerel, SP Crago, JP Walters</p>
    <p>Developed adaptive post-training quantization for transformer models in dynamic distributed edge environments.</p>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">ICASSP 2023</div>
      <img src="images/bebert.png" alt="BEBERT" width="80%">
    </div>
  </div>
  <div class='paper-box-text'>
    <h3><a href="https://ieeexplore.ieee.org/document/10095470">BEBERT: Efficient and Robust Binary Ensemble BERT</a></h3>
    <p>J Tian, C Fang, <strong>Haonan Wang</strong>, Z Wang</p>
    <p>Created an efficient and robust binary ensemble BERT, significantly reducing computational overhead.</p>
  </div>
</div>

<ul>
  <li><a href="https://ieeexplore.ieee.org/document/9401561">Accelerating 3D CNN using 3D Fast Fourier Transform</a>, C Fang, L He, <strong>Haonan Wang</strong>, J Wei, Z Wang, <em>ISCAS 2021</em></li>
  <li><a href="https://ieeexplore.ieee.org/document/9273915">Temporal Residual Feature Learning for 3D CNN Action Recognition</a>, <strong>Haonan Wang</strong>, Y Mei, J Lin, Z Wang, <em>SiPS 2020</em></li>
  <li><a href="https://arxiv.org/abs/1905.13388">Design Light-weight 3D CNN for Video Recognition</a>, <strong>Haonan Wang</strong>, J Lin, Z Wang, <em>arXiv 2019</em></li>
  <li><a href="https://ieeexplore.ieee.org/document/8683507">Sparse-Winograd Accelerator for CNNs</a>, <strong>Haonan Wang</strong>, W Liu, T Xu, J Lin, Z Wang, <em>ICASSP 2019</em></li>
  <li><a href="https://ieeexplore.ieee.org/document/8645271">Efficient Reconfigurable Hardware Core for CNNs</a>, <strong>Haonan Wang</strong>, J Lin, Y Xie, B Yuan, Z Wang, <em>Asilomar 2018</em></li>
</ul>


# üíª Internships
- *2024.06 - 2024.09*, **Research Scientist Intern**, Microsoft Azure Hardware System & Infrastructure, USA
  - Explored scaling laws of quantized LLM
  - Designed and trained BitNet models from 14M to 1B parameters
  - Implemented quantized LLM using micro-scaling quantization

- *2019.01 - 2020.05*, **Machine Learning Engineer Intern**, Windorise Tech. Co., China
  - Developed FPGA-based sparse CNN accelerators
  - Designed efficient 3D CNN algorithms for action recognition tasks

# üéñ Honors and Awards
- *2024* MLSys YPS Poster Session Presenter
- *2024* KESTON and ISI Exploratory Research Grants (\$100k)
- *2022* Research Festival of USC MHI ECE Best Poster Award
- *2020* USC Ph.D. Fellowship
- *2019* Best Poster Award & Travel Grant, Singapore AI Summer Workshop
- *2018* AI Scholarship, Nanjing University

# üìñ Educations
- *2020.09 - now*, Ph.D. Computer Engineering, Ming Hsieh Department of Electrical and Computer Engineering, Los Angeles, USA. 
- *2017.09 - 2020.06* M.Sc. Electronic Engineering, School of Electronic Science and Engineering, Nanjing University, China.
- *2013.09 - 2017.06* B.Sc. Physics, Kuang Yaming Honors School, Nanjing University, China.

# üí¨ Invited Talks
- *2024.05* MLSys Young Professionals Symposium, EFFICIENT AND TRUSTWORTHY DISTRIBUTED EDGEAI SYSTEM. 
